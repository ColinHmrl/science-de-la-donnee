{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "11BualWM3z2V"
   },
   "source": [
    "## 1.2 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install glob2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "umQTZ2yN3z2W",
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-17 11:20:12.060389: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-10-17 11:20:12.060444: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-10-17 11:20:12.060481: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-10-17 11:20:12.068761: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "\n",
    "# Configurations principales de nos modèles\n",
    "IMG_SIZE          = 224             # taille coté final d'une image en pixel (ici 28x28)\n",
    "NB_EPOCHS_DENOISE = 40               # nombre epoch alogithme debruiter\n",
    "BATCH_SIZE        = 8            # taille batch de traitement\n",
    "SAVE_MODEL_DENOISE = \"denoiser.h5\"     # sauvegarde du modele de debruitage\n",
    "\n",
    "def process(image):\n",
    "    image = tf.cast(image/255. ,tf.float32)\n",
    "    return image\n",
    "\n",
    "\n",
    "# Import du .env\n",
    "import dotenv\n",
    "import os\n",
    "\n",
    "# Chargement du .env !!!!!!!!!!!! CHANGER LE PATH !!!!!!!!!!!!!!\n",
    "# Renvoie true si le .env est chargé\n",
    "dotenv.load_dotenv('/home/cesi/datascience/.env.local')\n",
    "\n",
    "models_path = os.environ.get('MODELS_PATH_LIVRABLE2')\n",
    "sys.path.insert(0, models_path)\n",
    "\n",
    "import builder_vae\n",
    "import homemade\n",
    "import test2\n",
    "import resnet\n",
    "import test3\n",
    "\n",
    "\n",
    "SAVE_WEIGHTS_PATH = os.environ.get('WEIGHT_PATH_LIVRABLE2')\n",
    "SOURCE_LIVRABLE2_PATH = os.getenv(\"SOURCE_LIVRABLE2_PATH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9993 files belonging to 1 classes.\n",
      "Using 7995 files for training.\n",
      "Using 1998 files for validation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-17 11:21:56.540395: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-17 11:21:56.544745: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-17 11:21:56.544785: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-17 11:21:56.545423: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-17 11:21:56.545454: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-17 11:21:56.545477: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-17 11:21:57.209851: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-17 11:21:57.209898: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-17 11:21:57.209906: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1977] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-10-17 11:21:57.209942: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-17 11:21:57.209960: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6599 MB memory:  -> device: 0, name: Quadro P4000, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x_train, x_test = image_dataset_from_directory(\n",
    "    SOURCE_LIVRABLE2_PATH,\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_mode=\"categorical\",\n",
    "    # label_mode=None,\n",
    "    shuffle=False,\n",
    "    validation_split=0.2,\n",
    "    subset=\"both\",\n",
    "    seed=123,\n",
    "    color_mode=\"rgb\"\n",
    ")\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "x_train = x_train.map(lambda x,y: (x/255,y))\n",
    "x_test = x_test.map(lambda x,y: (x/255,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access './photoOnly/Photo': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "%ls ./photoOnly/Photo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dpI6DFBd3z2Y"
   },
   "source": [
    "Commençons par écrire une fonction qui permet de visualiser $n$ premiers enregistrements en noir et blanc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "oHzo3fUG3z2Y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from random import uniform\n",
    "import tensorflow as tf\n",
    "# os.chdir(r'/tf')\n",
    "from keras_cv.layers import RandomGaussianBlur\n",
    "\n",
    "def add_noise(img, perturbation_conf):\n",
    "    pertubation = perturbation_conf['perturbation']\n",
    "    noised_img = img\n",
    "    if pertubation == 1:\n",
    "        blur_kernel_value = perturbation_conf['blur_kernel_size']\n",
    "        noised_img = RandomGaussianBlur(kernel_size=blur_kernel_value, factor=(0.5, 3))(noised_img)\n",
    "    elif pertubation == 2:\n",
    "        noise_value = perturbation_conf['noise_factor']\n",
    "        noise = tf.random.normal(shape=tf.shape(img), mean=0.0, stddev=noise_value/255, dtype=tf.float32)\n",
    "        noised_img = tf.cast(img, tf.float32) + noise\n",
    "    elif pertubation == 3:\n",
    "        noise_value = perturbation_conf['noise_factor']\n",
    "        blur_kernel_value = perturbation_conf['blur_kernel_size']\n",
    "        noise = tf.random.normal(shape=tf.shape(img), mean=0.0, stddev=noise_value/255, dtype=tf.float32)\n",
    "        noised_img = tf.cast(img, tf.float32) + noise\n",
    "        noised_img = RandomGaussianBlur(kernel_size=blur_kernel_value, factor=(0.5, 1))(noised_img)\n",
    "    else:\n",
    "        noised_img = img\n",
    "        \n",
    "    return noised_img, img\n",
    "\n",
    "\n",
    "noise_configuration = {\n",
    "    0:{\n",
    "    'perturbation': 1,\n",
    "    'blur_kernel_size': 6,\n",
    "    },\n",
    "    1:{\n",
    "    'perturbation': 2,\n",
    "    'noise_factor': 20\n",
    "    },\n",
    "    2:{\n",
    "    'perturbation': 3,\n",
    "    'noise_factor': 12,\n",
    "    'blur_kernel_size': 2\n",
    "    },\n",
    "    3:{\n",
    "    'perturbation': 0\n",
    "    }\n",
    "}\n",
    "\n",
    "x_train_noisy = x_train.map(lambda x,y: (add_noise(x, noise_configuration[1])))\n",
    "x_test_noisy = x_test.map(lambda x,y: (add_noise(x, noise_configuration[1])))\n",
    "\n",
    "x_train_blur = x_train.map(lambda x,y: (add_noise(x, noise_configuration[0])))\n",
    "x_test_blur = x_test.map(lambda x,y: (add_noise(x, noise_configuration[0])))\n",
    "\n",
    "x_train_noise_blur = x_train.map(lambda x,y: (add_noise(x, noise_configuration[2])))\n",
    "x_test_noise_blur = x_test.map(lambda x,y: (add_noise(x, noise_configuration[2])))\n",
    "\n",
    "x_train_clean = x_train.map(lambda x,y: (add_noise(x, noise_configuration[3])))\n",
    "x_test_clean = x_test.map(lambda x,y: (add_noise(x, noise_configuration[3])))\n",
    "\n",
    "random_train_set = x_train.concatenate(x_train_noisy).concatenate(x_train_blur).concatenate(x_train_noise_blur)\n",
    "random_test_set = x_test.concatenate(x_test_noisy).concatenate(x_test_blur).concatenate(x_test_noise_blur)\n",
    "#shuffle the dataset\n",
    "# random_train_set = random_train_set.shuffle(1000)\n",
    "# random_test_set = random_test_set.shuffle(1000)\n",
    "\n",
    "\n",
    "# display three images on a single row with matplotlib \n",
    "def display_images(*images):\n",
    "    \"\"\"Display images on a single row.\"\"\"\n",
    "    plt.figure(figsize=(50, 50))\n",
    "    for index, image in enumerate(images):\n",
    "        plt.subplot(1, len(images), index+1)\n",
    "        plt.imshow(image)\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    # plt.figure(figsize=(15, 15))\n",
    "    # ax = plt.subplot(1, 3, 1)\n",
    "    # plt.imshow(left)\n",
    "    # plt.gray()\n",
    "    # ax.get_xaxis().set_visible(False)\n",
    "    # ax.get_yaxis().set_visible(False)\n",
    "    # ax = plt.subplot(1, 3, 2)\n",
    "    # plt.imshow(middle)\n",
    "    # plt.gray()\n",
    "    # ax.get_xaxis().set_visible(False)\n",
    "    # ax.get_yaxis().set_visible(False)\n",
    "    # if right is None:\n",
    "    #     return\n",
    "    # ax = plt.subplot(1, 3, 3)\n",
    "    # plt.imshow(right)\n",
    "    # plt.gray()\n",
    "    # ax.get_xaxis().set_visible(False)\n",
    "    # ax.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the same image from x_train and x_train_noisy and x_train_blur and x_train_noise_blur\n",
    "# for image, label in x_train.take(1):\n",
    "#     sample_image = image[0]\n",
    "#     sample_label = label\n",
    "# for image, label in x_train_noisy.take(1):\n",
    "#     sample_image_noisy = image[0]\n",
    "#     sample_label_noisy = label\n",
    "# for image, label in x_train_blur.take(1):\n",
    "#     sample_image_blur = image[0]\n",
    "#     sample_label_blur = label\n",
    "# for image, label in x_train_noise_blur.take(1):\n",
    "#     sample_image_noise_blur = image[0]\n",
    "#     sample_label_noise_blur = label\n",
    "# display_images(np.squeeze(sample_image), np.squeeze(sample_image_noisy), np.squeeze(sample_image_blur), np.squeeze(sample_image_noise_blur))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_uLTS74F3z2Z"
   },
   "source": [
    "Voyons ce que ça donne :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "_PC02d4V3z2s",
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)        [(None, 224, 224, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)          (None, 224, 224, 64)         1792      ['input_3[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)          (None, 224, 224, 64)         36928     ['conv2d_20[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)         (None, 224, 224, 64)         0         ['conv2d_21[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_22 (Conv2D)          (None, 224, 224, 128)        73856     ['dropout_2[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_23 (Conv2D)          (None, 224, 224, 128)        147584    ['conv2d_22[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_24 (Conv2D)          (None, 224, 224, 256)        295168    ['conv2d_23[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_25 (Conv2D)          (None, 224, 224, 128)        295040    ['conv2d_24[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_26 (Conv2D)          (None, 224, 224, 128)        147584    ['conv2d_25[0][0]']           \n",
      "                                                                                                  \n",
      " add_4 (Add)                 (None, 224, 224, 128)        0         ['conv2d_23[0][0]',           \n",
      "                                                                     'conv2d_26[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_27 (Conv2D)          (None, 224, 224, 64)         73792     ['add_4[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_28 (Conv2D)          (None, 224, 224, 64)         36928     ['conv2d_27[0][0]']           \n",
      "                                                                                                  \n",
      " add_5 (Add)                 (None, 224, 224, 64)         0         ['conv2d_28[0][0]',           \n",
      "                                                                     'conv2d_21[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_29 (Conv2D)          (None, 224, 224, 3)          1731      ['add_5[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1110403 (4.24 MB)\n",
      "Trainable params: 1110403 (4.24 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "MODEL_CHOSEN = 'test2'\n",
    "\n",
    "def load_model(model_choosen):\n",
    "    match(model_choosen):\n",
    "        case 'homemade':\n",
    "            model = homemade.build(IMG_SIZE)\n",
    "        case 'vae':\n",
    "            model = builder_vae.build(IMG_SIZE, 64)\n",
    "        case 'test2':\n",
    "            model = test2.build(IMG_SIZE)\n",
    "        case 'test3':\n",
    "            model = test3.build(IMG_SIZE)\n",
    "        case 'resnet':\n",
    "            model = resnet.build(IMG_SIZE)\n",
    "    return model\n",
    "autoencoder = load_model(MODEL_CHOSEN)\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K4w2DyFE3z3M"
   },
   "source": [
    "### Entrainement de l'auto-encodeur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BHnPwSlj3z3N"
   },
   "source": [
    "On va ensuite entraîner l'auto-encodeur en utilisant les constantes définit au début (`NB_EPOCHS_DENOISE,BATCH_SIZE`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(random_train_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "FLeiM12J7shx",
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/usr/local/lib/python3.11/dist-packages/keras/src/engine/training.py\", line 1377, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.11/dist-packages/keras/src/engine/training.py\", line 1360, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.11/dist-packages/keras/src/engine/training.py\", line 1349, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.11/dist-packages/keras/src/engine/training.py\", line 1131, in train_step\n        return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.11/dist-packages/keras/src/engine/training.py\", line 1225, in compute_metrics\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.11/dist-packages/keras/src/engine/compile_utils.py\", line 592, in update_state\n        self.build(y_pred, y_true)\n    File \"/usr/local/lib/python3.11/dist-packages/keras/src/engine/compile_utils.py\", line 498, in build\n        self._metrics = tf.__internal__.nest.map_structure_up_to(\n    File \"/usr/local/lib/python3.11/dist-packages/keras/src/engine/compile_utils.py\", line 646, in _get_metric_objects\n        return [self._get_metric_object(m, y_t, y_p) for m in metrics]\n    File \"/usr/local/lib/python3.11/dist-packages/keras/src/engine/compile_utils.py\", line 646, in <listcomp>\n        return [self._get_metric_object(m, y_t, y_p) for m in metrics]\n    File \"/usr/local/lib/python3.11/dist-packages/keras/src/engine/compile_utils.py\", line 667, in _get_metric_object\n        y_t_rank = len(y_t.shape.as_list())\n\n    ValueError: as_list() is not defined on an unknown TensorShape.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Administrateur\\Documents\\datascience\\project\\Livrable_2\\Auto_encodeur_non-redige.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/c%3A/Users/Administrateur/Documents/datascience/project/Livrable_2/Auto_encodeur_non-redige.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# path = createTrainingData.create_training_data(weight_path, model, model_choosen, num_classes, image_h, image_w, batch_size)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/c%3A/Users/Administrateur/Documents/datascience/project/Livrable_2/Auto_encodeur_non-redige.ipynb#X24sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell:/c%3A/Users/Administrateur/Documents/datascience/project/Livrable_2/Auto_encodeur_non-redige.ipynb#X24sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# checkpoint_path = path+\"/cp-{epoch:04d}.ckpt\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/Administrateur/Documents/datascience/project/Livrable_2/Auto_encodeur_non-redige.ipynb#X24sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/Administrateur/Documents/datascience/project/Livrable_2/Auto_encodeur_non-redige.ipynb#X24sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# history = model.fit(train_set, epochs=epochs, validation_data=test_set, callbacks=[weights_callback])\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Administrateur/Documents/datascience/project/Livrable_2/Auto_encodeur_non-redige.ipynb#X24sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m history \u001b[39m=\u001b[39m autoencoder\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/Administrateur/Documents/datascience/project/Livrable_2/Auto_encodeur_non-redige.ipynb#X24sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     random_train_set,\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/Administrateur/Documents/datascience/project/Livrable_2/Auto_encodeur_non-redige.ipynb#X24sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/Administrateur/Documents/datascience/project/Livrable_2/Auto_encodeur_non-redige.ipynb#X24sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/Administrateur/Documents/datascience/project/Livrable_2/Auto_encodeur_non-redige.ipynb#X24sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     validation_data\u001b[39m=\u001b[39;49mrandom_test_set\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/Administrateur/Documents/datascience/project/Livrable_2/Auto_encodeur_non-redige.ipynb#X24sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file_rwjxkdz.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.11/dist-packages/keras/src/engine/training.py\", line 1377, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.11/dist-packages/keras/src/engine/training.py\", line 1360, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.11/dist-packages/keras/src/engine/training.py\", line 1349, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.11/dist-packages/keras/src/engine/training.py\", line 1131, in train_step\n        return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.11/dist-packages/keras/src/engine/training.py\", line 1225, in compute_metrics\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.11/dist-packages/keras/src/engine/compile_utils.py\", line 592, in update_state\n        self.build(y_pred, y_true)\n    File \"/usr/local/lib/python3.11/dist-packages/keras/src/engine/compile_utils.py\", line 498, in build\n        self._metrics = tf.__internal__.nest.map_structure_up_to(\n    File \"/usr/local/lib/python3.11/dist-packages/keras/src/engine/compile_utils.py\", line 646, in _get_metric_objects\n        return [self._get_metric_object(m, y_t, y_p) for m in metrics]\n    File \"/usr/local/lib/python3.11/dist-packages/keras/src/engine/compile_utils.py\", line 646, in <listcomp>\n        return [self._get_metric_object(m, y_t, y_p) for m in metrics]\n    File \"/usr/local/lib/python3.11/dist-packages/keras/src/engine/compile_utils.py\", line 667, in _get_metric_object\n        y_t_rank = len(y_t.shape.as_list())\n\n    ValueError: as_list() is not defined on an unknown TensorShape.\n"
     ]
    }
   ],
   "source": [
    "# path = createTrainingData.create_training_data(weight_path, model, model_choosen, num_classes, image_h, image_w, batch_size)\n",
    "\n",
    "# checkpoint_path = path+\"/cp-{epoch:04d}.ckpt\"\n",
    "# checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# weights_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "#     filepath=checkpoint_path,\n",
    "#     verbose=1,\n",
    "#     save_weights_only=True,\n",
    "#     save_freq='epoch')\n",
    "\n",
    "# history = model.fit(train_set, epochs=epochs, validation_data=test_set, callbacks=[weights_callback])\n",
    "\n",
    "history = autoencoder.fit(\n",
    "    random_train_set,\n",
    "    epochs=4,\n",
    "    shuffle=True,\n",
    "    validation_data=random_test_set\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OZEugG4_AKzb"
   },
   "source": [
    "Affichez maintenant la courbe d'apprentissage. Que pensez-vous des performances du modèle ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_range = range()\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "\n",
    "plt.savefig(path+\"/metrics.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weight_path = SAVE_WEIGHTS_PATH + 'test2/weights_all_noise.h5' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.save_weights(model_weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.load_weights(model_weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = autoencoder.predict(x_test_noisy)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_index = 1\n",
    "batch_index = 1\n",
    "load_test_data_noised(1)\n",
    "original_image = list(x_test.take(batch_index).as_numpy_iterator())[0][0][image_index]\n",
    "noisy_image = list(x_test_noisy.take(batch_index).as_numpy_iterator())[0][0][image_index]\n",
    "denoised_image = pred[image_index]\n",
    "\n",
    "\n",
    "display_three_images(original_image, noisy_image, denoised_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = autoencoder.predict(list(x_test.take(1).as_numpy_iterator())[0][0][3:][0].reshape(1,224,224,3))\n",
    "\n",
    "display_images(list(x_test.take(1).as_numpy_iterator())[0][0][3:], n=1)\n",
    "display_images(p, n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "Oib6nOUB_t-A",
    "outputId": "5bbf833e-0879-4dd6-f290-3e4b604f971d"
   },
   "outputs": [],
   "source": [
    "# Visualisation des pertes d'apprentissage (Train) et de validation (Test)\n",
    "plt.plot( #A COMPLETER\n",
    "         label='train')\n",
    "plt.plot( #A COMPLETER\n",
    "         label='test')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "isdgF8Fw3z3W"
   },
   "source": [
    "Que pensez-vous des performances du modèle ?\n",
    "<em>À COMPLÉTER</em>\n",
    "\n",
    "\n",
    "# 1.4 Sauvgarde de l'auto-encodeur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lacVdNs93z3Y"
   },
   "source": [
    "L'entrainement de l'auto-encodeur sans utilisation de puissance de calcul (GPU) peut prendre beaucoup de temps. Usuellement, nous sauvegardons le modèle entraîné en local ou sur un serveur distant pour l'utiliser ultérieurement afin de traiter les nouvelles données (d'ailleurs, vous verrez une utilisation avancée de cette technique, le transfert learning, dès la semaine prochaine).\n",
    "Pour sauvegarder le modèle `autoencoder`, utiliser la méthode `save`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cH0sl-AZ3z3Z"
   },
   "outputs": [],
   "source": [
    "# save the model\n",
    "#A COMPLETER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YgZ7ckxd3z3a"
   },
   "outputs": [],
   "source": [
    "decoded_imgs = #A COMPLETER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 149
    },
    "id": "qjbGN2FX3z3a",
    "outputId": "b96c53b9-2af7-4f7d-8ab5-9fd55ae77504"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3IEX6qSH3z3b"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "sgKOfLjd3z3b"
   ],
   "name": "WS 2.3 - Autoencodeur et traitement d'image- Tuteur.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
