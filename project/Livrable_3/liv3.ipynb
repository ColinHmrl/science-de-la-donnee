{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "import"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from symspellpy import SymSpell\n",
    "import contractions\n",
    "import pickle\n",
    "\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "top_k = 5000 # nombre de mots à garder\n",
    "embedding_dim = 300 # dimension de l'embedding\n",
    "glove_dim = 300\n",
    "units = 64 # nombre d'unités GRU et le nombre d'unités pour l'attention de chaque layer\n",
    "\n",
    "BUFFER_SIZE = 1000\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "\n",
    "split = .4 # 4% des données pour le train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.abspath('.')\n",
    "base_path = \"/home/cesi/datascience/corbeille\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_captions = os.path.abspath('.')+\"/phase_6_7/annotations/captions_train2014.json\"\n",
    "train_captions = base_path+\"/annotations/captions_train2014.json\"\n",
    "\n",
    "# test_captions = os.path.abspath('.')+\"/phase_6_7/annotations/captions_val2014.json\"\n",
    "\n",
    "with open(train_captions, 'r') as f:\n",
    "    annotations = json.load(f)\n",
    "# with open(test_captions, 'r') as f:\n",
    "    # test_annotations = json.load(f)\n",
    "\n",
    "df = pd.DataFrame.from_dict(annotations['annotations'])\n",
    "# test_df = pd.DataFrame.from_dict(test_annotations['annotations'])\n",
    "\n",
    "# df['image_path'] = df.apply(lambda row: os.path.abspath('.')+\"/phase_6_7/train2014/COCO_train2014_\" + '%012d.jpg' % (row['image_id']), axis=1)\n",
    "df['image_path'] = df.apply(lambda row: base_path+\"/train2014/COCO_train2014_\" + '%012d.jpg' % (row['image_id']), axis=1)\n",
    "# test_df['image_path'] = test_df.apply(lambda row: os.path.abspath('.')+\"/phase_6_7/val2014/COCO_val2014_\" + '%012d.jpg' % (row['image_id']), axis=1)\n",
    "\n",
    "# split into train and validation\n",
    "\n",
    "train_df = df[:int(split*len(df))]\n",
    "# train_df.sample(frac=1)\n",
    "\n",
    "val_df = df[int((split)*len(df)):]\n",
    "print(train_df.count())\n",
    "#9s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print df index 1\n",
    "print(train_df)\n",
    "train_df.iloc[1]['image_path']\n",
    "img = plt.imread(train_df.iloc[1]['image_path'])\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep Data Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrait des mots ayant moins que deux caractères\n",
    "def remove_small(sentence):\n",
    "\n",
    "    words = sentence.split() \n",
    "    words = [word for word in words if len(word)>2]\n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "mots_vides=stopwords.words('english')\n",
    "def remove_stopwords(sentence):\n",
    "    \n",
    "    words = sentence.split()\n",
    "    words = [word for word in words if word not in mots_vides]\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "ponctuations = string.punctuation\n",
    "def remove_ponctuation(sentence):\n",
    "    \n",
    "    # print(ponctuations)\n",
    "    \n",
    "    for char in sentence:\n",
    "        if char in ponctuations:\n",
    "            sentence = sentence.replace(char, '')\n",
    "      \n",
    "    return sentence\n",
    "\n",
    "def nlp_pipeline(text):\n",
    "    # Convertir les lettres majuscules en minuscules\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remplacer la nouvelle ligne par un espace\n",
    "    text = text.replace('\\n', ' ').replace('\\r', '')\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    # Retirer les lettres majuscules, toutes les chaine de caractères qui ne sont pas des lettres ou des chiffres\n",
    "    #À COMPLÉTER\n",
    "    \n",
    "    # Retirer les caractères spéciaux\n",
    "    text = re.sub(r\"(\\s\\-\\s|-$)\", \"\", text)       \n",
    "    #À COMPLÉTER\n",
    "    text = re.sub(r\"\\x89û\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def add_start_end_seq_token(caption):\n",
    "    caption = f\"<start> {caption} <end>\" \n",
    "    return caption\n",
    "\n",
    "# def remove_too_long_text(text):\n",
    "#     return len(text.split()) < 30\n",
    "\n",
    "def expand_contractions(text):\n",
    "    return contractions.fix(text)\n",
    "\n",
    "symsp = SymSpell()\n",
    "symsp.load_dictionary('/home/cesi/datascience/data/frequency_dictionary.txt', term_index=0, count_index=1, separator=' ')\n",
    "\n",
    "def correct_spelling(text):\n",
    "    suggestions = symsp.lookup_compound(text, max_edit_distance=2)\n",
    "    return suggestions[0].term\n",
    "\n",
    "def clean_text(data):\n",
    "    # data['text'] = data['text'].apply(lambda x : remove_url(x))\n",
    "    # data['text'] = data['text'].apply(lambda x : remove_html(x))\n",
    "    # data['text'] = data['text'].apply(lambda x : stem_words(x))\n",
    "    # data['caption'] = data['caption'].apply(lambda x : remove_ponctuation(x))\n",
    "    # data['caption'] = data['caption'].apply(lambda x : remove_stopwords(x))\n",
    "    # data['text'] = data['text'].apply(lambda x : remove_emoji(x))\n",
    "    # data['caption'] = data['caption'].apply(lambda x : remove_small(x))\n",
    "    # data['text'] = data['text'].apply(lambda x : lem_word(x))\n",
    "    data['caption'] = data['caption'].apply(lambda x : nlp_pipeline(x))\n",
    "    data['caption'] = data['caption'].apply(lambda x : correct_spelling(x))\n",
    "    data['caption'] = data['caption'].apply(lambda x : expand_contractions(x))\n",
    "    data['caption'] = data['caption'].apply(lambda x : add_start_end_seq_token(x))\n",
    "\n",
    "    return data\n",
    "\n",
    "train_df = clean_text(train_df)\n",
    "\n",
    "#20s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokeniser && Data Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer(df):\n",
    "    def calc_max_length(tensor):\n",
    "        return max(len(t) for t in tensor)\n",
    "\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "                                                    # num_words=top_k,\n",
    "                                                    oov_token=\"<unk>\",\n",
    "                                                    filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
    "\n",
    "    tokenizer.fit_on_texts(df['caption'].tolist())\n",
    "    tokenizer.word_index['<pad>'] = 0\n",
    "    tokenizer.index_word[0] = '<pad>'\n",
    "\n",
    "    train_seqs = tokenizer.texts_to_sequences(df['caption'].tolist())\n",
    "    cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')\n",
    "    # max_length = calc_max_length(train_seqs)\n",
    "    max_length = 23\n",
    "\n",
    "    return tokenizer, cap_vector, max_length\n",
    "\n",
    "tokenizer, cap_vector, max_length = get_tokenizer(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taille du vocabulaire\n",
    "print(len(tokenizer.word_index))\n",
    "\n",
    "#Liste triée dans l'ordre de la fréquence décroissante\n",
    "print(sorted(list(tokenizer.word_counts.items()),key=lambda x: -x[1])[:20])\n",
    "\n",
    "print(cap_vector[0]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstrcut sentence of cap_vector[0] using tokenizer\n",
    "print(\" \".join([tokenizer.index_word[i] for i in cap_vector[0] if i not in [0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dict = {}\n",
    "\n",
    "f=open(os.path.join(base_path, 'glove', 'glove.6B.'+str(glove_dim)+'d.txt'),'r',encoding='utf-8')\n",
    "for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vectors = np.asarray(values[1:],'float32')\n",
    "        # Transformer chaque mot en un vecteur de dimensions 100.\n",
    "        embedding_dict[word]=vectors\n",
    "        \n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Matrice initiale emmbedding pour notre jeu de données\n",
    "hit=0\n",
    "misses={}\n",
    "\n",
    "# Nombre de token ( numpy est à base zéro)\n",
    "num_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Dimension de représentation =100 selon Glove choisi\n",
    "embedding_matrix = np.zeros((num_words, glove_dim))\n",
    "\n",
    "# On remplit la matrice avec les coordonnées issues de la représentation pré-entrainée\n",
    "# Pourvu que le terme de notre dictionnaire recherché soit présent dans la représentation pré-entrainée de GloVe\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i > num_words:\n",
    "        continue\n",
    "    \n",
    "    emb_vec = embedding_dict.get(word)\n",
    "    \n",
    "    if emb_vec is not None:\n",
    "        embedding_matrix[i] = emb_vec\n",
    "        # print(embedding_matrix[i])\n",
    "        hit = hit+1\n",
    "    else:\n",
    "        if word not in misses.keys():\n",
    "            misses[word] = 1\n",
    "        else:\n",
    "            misses[word] += 1\n",
    "        \n",
    "# Affichage de controle: le nombre de termes trouvés et non trouvés dans la représentation pré-entrainée de GloVe.\n",
    "\n",
    "print('Trouvés %s mots.' % hit)\n",
    "print('Non trouvés %s mots.' % len(misses))\n",
    "\n",
    "new_list = sorted(misses, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show top most 10 words not found in glove from the object misses created above\n",
    "print(sorted(misses.items(), reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep Data Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Telechargement du modèle InceptionV3 pré-entrainé avec la cassification sur ImageNet\n",
    "image_model = tf.keras.applications.InceptionV3(include_top=False,\n",
    "                                                weights='imagenet')\n",
    "for layer in image_model.layers:\n",
    "    layer.trainable = False\n",
    "# Creation d'une variable qui sera l'entrée du nouveau modèle de pre-traitement d'images\n",
    "new_input = image_model.input\n",
    "# récupérer la dernière couche caché qui contient l'image en representation compacte\n",
    "hidden_layer = image_model.layers[-1].output\n",
    "\n",
    "\n",
    "# Modèle qui calcule une representation dense des images avec InceptionV3\n",
    "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)\n",
    "\n",
    "# Définition de la fonction load_image\n",
    "def load_image(image_path):\n",
    "    \"\"\"\n",
    "    La fonction load_image a pour entrée le chemin d'une image et pour sortie un couple\n",
    "    contenant l'image traitée ainsi que son chemin dtop_k = 5000accès.\n",
    "    La fonction load_image effectue les traitement suivant:\n",
    "        1. Chargement du fichier correspondant au chemin d'accès image_path\n",
    "        2. Décodage de l'image en RGB.\n",
    "        3. Redimensionnement de l'image en taille (299, 299).\n",
    "        4. Normalisation des pîxels de l'image entre -1 et 1\n",
    "    \"\"\"\n",
    "    img = tf.io.read_file(image_path)  # 1. Chargement du fichier\n",
    "    img = tf.image.decode_jpeg(img, channels=3)  # 2. Décodage en RGB\n",
    "    img = tf.image.resize(img, (299, 299))  # 3. Redimensionnement à 299x299\n",
    "    img = tf.keras.applications.inception_v3.preprocess_input(img)  # 4. Normalisation entre -1 et 1\n",
    "    return img, image_path \n",
    "\n",
    "# Pré-traitement des images\n",
    "# Prendre les noms des images\n",
    "encode_train = sorted(set(train_df['image_path']))\n",
    "\n",
    "# Creation d'une instance de \"tf.data.Dataset\" partant des noms des images \n",
    "image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n",
    "# Division du données en batchs après application du pré-traitement fait par load_image\n",
    "image_dataset = image_dataset.map(\n",
    "  load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(16)\n",
    "\n",
    "# Parcourir le dataset batch par batch pour effectuez le pré-traitement d'InceptionV3\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_pickle('/home/cesi/datascience/data/captions_train2014_souf.pkl')\n",
    "# def change_path(path):\n",
    "#     return path.replace('/tf', base_path)\n",
    "# df['image_path'] = df['image_path'].apply(lambda x : change_path(x))\n",
    "# df.to_pickle('/home/cesi/datascience/data/captions_train2014_souf.pkl')\n",
    "\n",
    "# for img, path in tqdm(image_dataset):\n",
    "#     # Pré-traitement du batch (de taille (16,8,8,2048)) courant par InceptionV3 \n",
    "#     batch_features = image_features_extract_model(img)\n",
    "#     # Resize du batch de taille (16,8,8,2048) en taille (16,64,2048)\n",
    "#     batch_features = tf.reshape(batch_features,\n",
    "#                               (batch_features.shape[0], -1, batch_features.shape[3]))\n",
    "#     # Parcourir le batch courant et stocker le chemin ainsi que le batch avec np.save()\n",
    "#     for bf, p in zip(batch_features, path):\n",
    "#         path_of_feature = p.numpy().decode(\"utf-8\")\n",
    "#         # (chemin de l'image associe a sa nouvelle representation , representation de l'image)\n",
    "#         np.save(path_of_feature, bf.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_func(img_name, cap):\n",
    "    img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n",
    "    return img_tensor, cap\n",
    "\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((train_df['image_path'].values, cap_vector ))\n",
    "dataset = dataset.map(lambda item1, item2: tf.numpy_function(\n",
    "          map_func, [item1, item2], [tf.float32, tf.int32]),\n",
    "          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Melanger les donnees et les diviser en batchs\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(dataset.as_numpy_iterator())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(dataset.take(1).as_numpy_iterator())[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encodeur\n",
    "CNN + dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Encoder(tf.keras.Model):\n",
    "    # Comme les images sont déjà prétraités par InceptionV3 est représenté sous forme compacte\n",
    "    # L'encodeur CNN ne fera que transmettre ces caractéristiques à une couche dense\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        # forme après fc == (batch_size, 64, embedding_dim)\n",
    "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)  # Transformation pour features\n",
    "        self.W2 = tf.keras.layers.Dense(units)  # Transformation pour hidden\n",
    "        self.V = tf.keras.layers.Dense(1)  # Transformation pour score\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        # Ajout d'une dimension de temps à hidden pour le calcul\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "        attention_hidden_layer = (tf.nn.tanh(self.W1(features) +\n",
    "                                             self.W2(hidden_with_time_axis)))\n",
    "        \n",
    "        # Calcul du score\n",
    "        score = self.V(attention_hidden_layer)\n",
    "\n",
    "        # Poids d'attention\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # Vecteur de contexte\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        #context_vector est \n",
    "\n",
    "        return context_vector, attention_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RNN_Decoder(tf.keras.Model):\n",
    "    def __init__(self, units, vocab_size):\n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(\n",
    "                                        len(tokenizer.word_index)+1,\n",
    "                                        embedding_dim,\n",
    "                                        embeddings_initializer = tf.keras.initializers.Constant(embedding_matrix),\n",
    "                                        trainable=False\n",
    "                                    )\n",
    "\n",
    "\n",
    "        self.gru = tf.keras.layers.GRU(self.units,  \n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "        self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(self.units)\n",
    "\n",
    "    def call(self, x, features, hidden):\n",
    "        # print(\"input decoder:\",x.shape)\n",
    "        # print(x)\n",
    "    \n",
    "        context_vector, attention_weights = self.attention(features, hidden)\n",
    "        # print(\"context_vector.shape = \", context_vector.shape)\n",
    "        # print(\"attention_weights.shape = \", attention_weights.shape)\n",
    "        # Passage du mot courant à la couche embedding\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # print(\"post embedding:\",x.shape)\n",
    "        # print(x)\n",
    "        # Concaténation du vecteur de contexte et du mot courant\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        \n",
    "        # print(\"post concat:\",x.shape)\n",
    "        # print(x)\n",
    "\n",
    "        # Passage du vecteur concaténé à la gru\n",
    "        output, state = self.gru(x)\n",
    "        \n",
    "        # print(\"post gru:\",output.shape)\n",
    "        # print(output)\n",
    "        # print(\"post gru state:\",state.shape)\n",
    "        # print(state)\n",
    "\n",
    "        # Couche dense intermédiaire\n",
    "        y = self.fc1(output)\n",
    "        \n",
    "        y = tf.reshape(y, (-1, y.shape[2]))\n",
    "        \n",
    "        \n",
    "        # Dernière couche dense pour prédire le prochain mot du vocabulaire\n",
    "        y = self.fc2(y)\n",
    "\n",
    "        return y, state, attention_weights\n",
    "\n",
    "    def reset_state(self, batch_size):\n",
    "        # print(\"reset_state:batch_size = \", batch_size)\n",
    "        return tf.zeros((batch_size, self.units))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tokenizer.word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = CNN_Encoder(embedding_dim)\n",
    "# Création du décodeur\n",
    "decoder = RNN_Decoder(units, len(tokenizer.word_index))\n",
    "\n",
    "\n",
    "# Optimiseur ADAM\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# La fonction de perte\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint todo\n",
    "# checkpoint_path = \"./checkpoints/train\"\n",
    "# ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "#                            decoder=decoder,\n",
    "#                            optimizer = optimizer)\n",
    "# ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "\n",
    "# start_epoch = 0\n",
    "# if ckpt_manager.latest_checkpoint:\n",
    "#     start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
    "#     # Restaurer le dernier checkpoint dans checkpoint_path\n",
    "#     ckpt.restore(ckpt_manager.latest_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#debug\n",
    "\n",
    "for (batch, (img_tensor, target)) in enumerate(dataset):\n",
    "    # print(target.shape[0])\n",
    "    # print(target[0])\n",
    "    # print([tokenizer.word_index['<start>']] * target.shape[0])\n",
    "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n",
    "    # print(dec_input[0])\n",
    "    # print(img_tensor.shape)\n",
    "    features = encoder(img_tensor)\n",
    "    # print(features.shape)\n",
    "\n",
    "\n",
    "   \n",
    "    predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "\n",
    "    print(predictions.shape)\n",
    "    print(target.shape)\n",
    "\n",
    "    break\n",
    "    # print([tokenizer.word_index['<start>']] *  target.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int(dataset.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_plot = []\n",
    "@tf.function\n",
    "def train_step(img_tensor, target):\n",
    "    loss = 0\n",
    "\n",
    "    # Initialisation de l'état caché pour chaque batch\n",
    "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "    \n",
    "    # Initialiser l'entrée du décodeur\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n",
    "    \n",
    "    \n",
    "\n",
    "    with tf.GradientTape() as tape: # Offre la possibilité de calculer le gradient du loss\n",
    "        features = encoder(img_tensor)\n",
    "\n",
    "        for i in range(1, target.shape[1]):\n",
    "            # Prédiction des i'èmes mot du batch avec le décodeur\n",
    "            predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "            loss += loss_function(target[:, i], predictions)\n",
    "\n",
    "            # Le mot correct à l'étap i est donné en entrée à l'étape (i+1)\n",
    "            dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "    total_loss = (loss / int(target.shape[1]))\n",
    "\n",
    "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return loss, total_loss\n",
    "\n",
    "EPOCHS = 10\n",
    "# val_dataset = dataset[:len(dataset)//20]\n",
    "# dataset = dataset[len(dataset)//20:]\n",
    "\n",
    "for epoch in range(0, EPOCHS): # start_epoch, EPCHOS\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for (batch, (img_tensor, target)) in tqdm(enumerate(dataset)):\n",
    "        # print(\"batch = \", batch)\n",
    "        # print(\"img_tensor = \", img_tensor)\n",
    "        # print(\"target = \", target)\n",
    "        \n",
    "        batch_loss, t_loss = train_step(img_tensor, target)\n",
    "        total_loss += t_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print ('Epoch {} Batch {} Loss {:.4f}'.format(\n",
    "              epoch + 1, batch, batch_loss.numpy() / int(target.shape[1])))\n",
    "            # break\n",
    "        \n",
    "    \n",
    "    # sauvegarde de la perte\n",
    "    loss_plot.append(total_loss / int(dataset.__len__()))\n",
    "    \n",
    "    # if epoch % 5 == 0:\n",
    "    #     ckpt_manager.save()\n",
    "\n",
    "    print ('Epoch {} Loss {:.6f}'.format(epoch + 1,\n",
    "                                         total_loss/int(dataset.__len__())))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "\n",
    "# Affichage de la courbe d'entrainement\n",
    "plt.plot(loss_plot)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage de la courbe d'entrainement\n",
    "plt.plot(loss_plot)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_features_shape = 64\n",
    "\n",
    "def evaluate_loss(image, label):\n",
    "    hidden = decoder.reset_state(batch_size=1)\n",
    "\n",
    "    temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
    "    img_tensor_val = image_features_extract_model(temp_input)\n",
    "    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
    "\n",
    "    features = encoder(img_tensor_val)\n",
    "\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
    "    result = []\n",
    "\n",
    "    for i in range(max_length):\n",
    "        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
    "\n",
    "        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
    "        result.append(tokenizer.index_word[predicted_id])\n",
    "\n",
    "        if tokenizer.index_word[predicted_id] == '<end>':\n",
    "            break\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    loss = loss_function(label[:, i], predictions)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def evaluate(image):\n",
    "    attention_plot = np.zeros((max_length, attention_features_shape))\n",
    "\n",
    "    hidden = decoder.reset_state(batch_size=1)\n",
    "\n",
    "    temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
    "    img_tensor_val = image_features_extract_model(temp_input)\n",
    "    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
    "\n",
    "    features = encoder(img_tensor_val)\n",
    "\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
    "    result = []\n",
    "\n",
    "    for i in range(max_length):\n",
    "        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
    "\n",
    "        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
    "\n",
    "        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
    "        result.append(tokenizer.index_word[predicted_id])\n",
    "\n",
    "        if tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, attention_plot\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    attention_plot = attention_plot[:len(result), :]\n",
    "    return result, attention_plot\n",
    "\n",
    "import math\n",
    "# Fonction permettant la représentation de l'attention au niveau de l'image\n",
    "# def plot_attention(image, result, attention_plot):\n",
    "#     temp_image = np.array(Image.open(image))\n",
    "\n",
    "#     fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "#     len_result = len(result)\n",
    "#     for l in range(len_result):\n",
    "#         temp_att = np.resize(attention_plot[l], (8, 8))\n",
    "#         ax = fig.add_subplot(len_result//2, len_result//2, l+1)\n",
    "#         ax.set_title(result[l])\n",
    "#         img = ax.imshow(temp_image)\n",
    "#         ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "def plot_attention(image, result, attention_plot):\n",
    "    temp_image = np.array(Image.open(image))\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    \n",
    "    len_result = len(result)\n",
    "    grid_size = math.ceil(math.sqrt(len_result))\n",
    "    \n",
    "    for l in range(len_result):\n",
    "        temp_att = np.resize(attention_plot[l], (8, 8))\n",
    "        ax = fig.add_subplot(grid_size, grid_size, l+1)\n",
    "        ax.set_title(result[l])\n",
    "        img = ax.imshow(temp_image)\n",
    "        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n",
    "    plt.savefig(\"/home/cesi/datascience/project/Livrable_3/attention.png\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Affichage de quelques annotations dans le jeu de test\n",
    "# rid = np.random.randint(0, len(img_name_val))\n",
    "\n",
    "rid = 10\n",
    "\n",
    "# image = img_name_val[rid]\n",
    "image = df.iloc[rid]['image_path']\n",
    "\n",
    "# real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\n",
    "real_caption = df.iloc[rid]['caption']\n",
    "\n",
    "result, attention_plot = evaluate(image)\n",
    "\n",
    "print ('Real Caption:', real_caption)\n",
    "print ('Prediction Caption:', ' '.join(result))\n",
    "plot_attention(image, result, attention_plot)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
